{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff442b39-8c8f-4bc1-84f1-8ab3bddf78d4",
   "metadata": {},
   "source": [
    "## Setting Up Logistic Regression for Text Classification\n",
    "\n",
    "The following code is part of a machine learning pipeline that processes, analyzes, and classifies text data from a dataset containing newsgroup posts. Here's a summary of the main steps and components:\n",
    "\n",
    "1. **Imports**: The script begins by importing necessary Python libraries and modules for data manipulation, machine learning, and plotting. This includes `pandas`, `numpy` for data handling, `datasets` for loading the data, `matplotlib` for visualization, and several components from `scikit-learn` for text vectorization, dimensionality reduction, and logistic regression modeling. Additional libraries for text preprocessing such as `re` and `nltk` are also imported.\n",
    "\n",
    "2. **Data Loading**: The dataset named \"rungalileo/20_Newsgroups_Fixed\" is loaded using the `datasets` library. This dataset contains posts from 20 different newsgroups, along with labels indicating which newsgroup each post belongs to.\n",
    "\n",
    "3. **Data Preprocessing**:\n",
    "    - Text cleaning functions are introduced to remove URLs, user references, hashtags, numbers, punctuation, underscores, repeated letters, and to apply lowercasing, lemmatization, and stop word removal.\n",
    "    - Entries where the label or text is missing are filtered out.\n",
    "    - The cleaned and validated text and labels (`y_train` and `y_test`) are then prepared for vectorization.\n",
    "\n",
    "4. **Text Vectorization**:\n",
    "    - The `TfidfVectorizer` is configured with thresholds (`min_df=5`, `max_df=0.40`) to vectorize the cleaned text data. This is applied to both the training and test datasets.\n",
    "\n",
    "5. **Model Training and Prediction**:\n",
    "    - A logistic regression model is set up with a fixed random state. The model is trained using the vectorized training data and labels.\n",
    "    - The model's performance is then evaluated using the test data, and metrics such as accuracy, precision, recall, and F1 score are calculated and displayed.\n",
    "\n",
    "6. **Evaluation Metrics**:\n",
    "    - The code includes functions to print various classification metrics to assess the performance of the logistic regression model.\n",
    "\n",
    "This process involves typical steps in a text classification task, including comprehensive data cleaning, feature extraction through TF-IDF vectorization, and classification using logistic regression. The updated code focuses significantly on the preprocessing of text and evaluation of model performance, illustrating a robust approach to handling and classifying textual data.\n",
    "\n",
    "**Note**: The primary focus of this post is on visualizing the results of multiclass text classification using logistic regression. The subsequent sections will cover techniques for visualizing the model's performance and the data distribution in reduced dimensions. Also, no hyperparameter tuning is performed for the logistic regression model. The emphasis is on the visualization aspect of the analysis.\n",
    "\n",
    "[GitHub Repository for this post](https://github.com/trenton3983/Blog-Posts/blob/b0e9f77ff794597392cca4a32348845454cbc3ec/2024-04-29-visualizing-logistic-regression-multiclass-text-data/2024-04-29-visualizing-logistic-regression-multiclass-text-data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96872af-2dd7-4e9a-805c-0eb9a36fa0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports at the top\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation (optional, based on your needs)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove underscores\n",
    "    text = re.sub(r'_+', ' ', text)  # Replace one or more underscores with a space\n",
    "    # Remove repeated letters (more than 2)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)  # Reduce occurrences of the same character to two\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Lemmatization and stop words removal\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "\n",
    "def print_evaluation_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Print the evaluation metrics for classification.\n",
    "    \"\"\"\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "def clean_and_update(entry):\n",
    "    # Assuming clean_text function is defined to clean the text\n",
    "    if entry['text'] is not None:\n",
    "        entry['text'] = clean_text(entry['text'])\n",
    "    return entry  # Return the modified entry as a dict\n",
    "\n",
    "\n",
    "def is_valid(entry):\n",
    "    # Check if both 'label' and 'text' are not None\n",
    "    return entry['label'] is not None and entry['text'] is not None\n",
    "\n",
    "\n",
    "# Load the data\n",
    "dataset = load_dataset(\"rungalileo/20_Newsgroups_Fixed\")\n",
    "\n",
    "# Apply cleaning to both train and test datasets\n",
    "filtered_dataset = {\n",
    "    'train': dataset['train'].map(clean_and_update),\n",
    "    'test': dataset['test'].map(clean_and_update)\n",
    "}\n",
    "\n",
    "# Apply filtering to both train and test datasets\n",
    "filtered_dataset = {\n",
    "    'train': filtered_dataset['train'].filter(is_valid),\n",
    "    'test': filtered_dataset['test'].filter(is_valid)\n",
    "}\n",
    "\n",
    "y_train = filtered_dataset['train']['label']\n",
    "y_test = filtered_dataset['test']['label']\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.40)\n",
    "x_train = vectorizer.fit_transform(filtered_dataset['train']['text'])\n",
    "x_test = vectorizer.transform(filtered_dataset['test']['text'])\n",
    "\n",
    "# Set up and train the logistic regression model\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "log_pred = logreg.predict(x_test)\n",
    "\n",
    "# Display feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f'Feature Names: {feature_names}\\nNumber of Feature Names: {len(feature_names)}\\n')\n",
    "\n",
    "\n",
    "# Print out some evaluation metrics\n",
    "print_evaluation_metrics(y_test, log_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52ab13-4516-4f4a-85f9-27c68d8e613e",
   "metadata": {},
   "source": [
    "## Multiclass Visualization\n",
    "\n",
    "Significant challenges arise when visualizing logistic regression results for a multiclass classification task, especially with text data transformed via [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) vectorization. Traditional 2D or 3D plots used in binary logistic regression, [such as this example](https://raw.githubusercontent.com/trenton3983/Blog-Posts/main/2024-04-29-visualizing-logistic-regression-multiclass-text-data/82L2cb2T.png), are not directly applicable because they typically represent binary outcomes and cannot easily accommodate the high-dimensional space created by TF-IDF vectorization.\n",
    "\n",
    "### Key Challenges:\n",
    "\n",
    "1. **High Dimensionality**: TF-IDF vectorization transforms text into a high-dimensional space (often thousands of dimensions), where each dimension corresponds to a specific word's frequency or importance in the text. Standard binary logistic regression plots, which typically show decision boundaries in two or three dimensions, cannot naturally extend to this high-dimensional space.\n",
    "\n",
    "2. **Multiclass Classification**: Binary logistic regression is inherently designed for two classes. In multiclass settings, logistic regression models typically use schemes like [one-vs-rest](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) (OvR) or [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), which involve multiple binary decisions or probability distributions across more than two classes. This complexity makes it challenging to depict decision boundaries or class separations in a simple 2D or 3D plot.\n",
    "\n",
    "### Alternative Visualization Strategies:\n",
    "\n",
    "Given these constraints, alternative visualization methods are recommended:\n",
    "\n",
    "- [**Confusion Matrix**](https://en.wikipedia.org/wiki/Confusion_matrix): This is particularly useful in multiclass settings to show the model's performance across all classes, illustrating how often each class is correctly predicted versus misclassified.\n",
    "\n",
    "- [**Dimensionality Reduction**](https://en.wikipedia.org/wiki/Dimensionality_reduction): Techniques such as [PCA](https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis_(PCA)) (Principal Component Analysis) or [t-SNE](https://en.wikipedia.org/wiki/Dimensionality_reduction#t-SNE) (t-Distributed Stochastic Neighbor Embedding) can be used to reduce the dataset to two or three dimensions. These reduced dimensions can then be visualized in a scatter plot, providing a way to observe data clustering and separation at a high level.\n",
    "\n",
    "These methods provide more meaningful insights into the performance and behavior of logistic regression models in multiclass, high-dimensional scenarios like those involving TF-IDF vectorized text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a9e49-4b58-47a5-bc97-b2aef92a7627",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix can help you understand the performance of your classifier across different classes. It shows the actual vs. predicted classifications.\n",
    "\n",
    "In a confusion matrix for a classification task, the matrix isn't necessarily symmetrical, meaning the upper and lower sections (above and below the diagonal) aren't expected to be mirror images of each other. Here’s why:\n",
    "\n",
    "### Understanding the Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the counts of predictions versus the actual labels:\n",
    "- **Diagonal elements** show the number of correct predictions for each class (True Positives for each class).\n",
    "- **Off-diagonal elements** show the misclassifications:\n",
    "  - **Elements above the diagonal** indicate how many times class X was incorrectly predicted as class Y.\n",
    "  - **Elements below the diagonal** indicate how many times class Y was incorrectly predicted as class X.\n",
    "\n",
    "### Reasons for Asymmetry\n",
    "\n",
    "1. **Class Imbalance**: If some classes have more samples than others, the likelihood of predicting the majority class increases, affecting the symmetry.\n",
    "\n",
    "2. **Model Biases and Sensitivities**: The model might be better at recognizing certain classes over others due to inherent biases in the training data or differences in feature distinctiveness between classes.\n",
    "\n",
    "3. **Error Types**:\n",
    "  - [**Type I Errors (False Positives)**](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error): Cases where the model incorrectly predicts the positive class.\n",
    "  - [**Type II Errors (False Negatives)**](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_II_error): Cases where the model fails to predict the positive class when it is the actual class.\n",
    "    Each type of error might be more common for certain classes than others.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose you have three classes: A, B, and C. If:\n",
    "- Class A is often confused with Class B, but not vice versa.\n",
    "- Class C is often mistaken for both A and B, but rarely are A or B mistaken for C.\n",
    "\n",
    "This leads to a non-symmetrical confusion matrix because the misclassification patterns are not uniform across classes.\n",
    "\n",
    "### Interpreting Asymmetries in the Confusion Matrix\n",
    "\n",
    "A non-symmetrical confusion matrix is typical in practice, especially in multi-class scenarios where varying features, class distributions, and model sensitivities contribute to unique patterns of misclassification. This matrix is a valuable tool for identifying how well the model performs on each class and where it may need improvements or adjustments in its training data or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ef7f4-5d43-4213-ae68-e04ede3559ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, log_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(ax=ax, cmap='viridis', xticks_rotation='vertical')  # You can specify the color map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b4f49-7907-49b9-bcb3-b9d5aa4a7c35",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction for Visualization\n",
    "\n",
    "You can use techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of your TF-IDF vectors to two or three dimensions, and then plot these dimensions.\n",
    "\n",
    "### General PCA Explanation\n",
    "\n",
    "**Principal Components**: Each principal component is a linear combination of the original features and represents a direction in the feature space where the data varies the most. Principal components are orthogonal to each other and are ordered by the amount of variance they capture from the data.\n",
    "\n",
    "- **Variance Representation**: The first principal component captures the most variance, the second captures the next most, and so on, under the constraint that each is orthogonal to the others.\n",
    "- **Interpretation of Axes**: The axes in PCA plots (whether 2D or 3D) represent these principal components. The ticks on these axes indicate the scale or magnitude of the data points along the respective principal components. Since the data often undergoes transformation such as scaling to have zero mean and unit variance before applying PCA, these ticks do not represent the original units of measurement but rather relative positions in the transformed space.\n",
    "\n",
    "### Specific to 2D Plots\n",
    "\n",
    "In a 2D PCA plot:\n",
    "\n",
    "- **X-axis (First Principal Component)**: Represents the direction of the greatest variance in the data. This axis captures the largest amount of information (variation) in the dataset.\n",
    "- **Y-axis (Second Principal Component)**: Represents the direction of the second greatest variance, orthogonal to the first principal component.\n",
    "\n",
    "**Visual Analysis**: The plot can reveal clustering and other patterns, helping to visually assess similarities and differences in the data. Points that are close together can be interpreted as having similar characteristics according to the most significant features.\n",
    "\n",
    "### Specific to 3D Plots\n",
    "\n",
    "In addition to the first and second principal components, 3D PCA plots include:\n",
    "\n",
    "- **Z-axis (Third Principal Component)**: This axis captures the third highest variance in the data, providing an additional dimension of analysis which is orthogonal to both the first and second components.\n",
    "\n",
    "**Enhanced Visual Analysis**: A 3D plot allows for a deeper visual exploration, revealing structures and relationships that might not be visible in 2D. It can be especially useful in datasets where the top two components do not capture the majority of variance.\n",
    "\n",
    "### Practical Use\n",
    "\n",
    "Both 2D and 3D PCA plots are used for:\n",
    "- **Pattern Recognition**: Identifying clusters or outliers in the data.\n",
    "- **Data Simplification**: Reducing the dimensionality of the data while attempting to retain the most important characteristics.\n",
    "- **Exploratory Data Analysis**: Providing insights into the structure of the data before applying more complex models.\n",
    "\n",
    "These visualizations serve as powerful tools for initial data analysis, especially when dealing with complex datasets like those generated from text vectorization (e.g., TF-IDF) in natural language processing tasks. They help in making informed decisions about the next steps in the data analysis or machine learning workflow.\n",
    "\n",
    "#### 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b125f-fd86-4732-822e-76c2f6b27d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions (PCA)\n",
    "label_encoder = LabelEncoder()\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_test_numeric, cmap='viridis')\n",
    "plt.colorbar(scatter) \n",
    "\n",
    "# Generate legend\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "plt.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a47d18-5698-4c80-bec7-3c920ecbca0a",
   "metadata": {},
   "source": [
    "#### 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e4ef2-89ac-47e3-8467-e9648351e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform with PCA for 3 components\n",
    "label_encoder = LabelEncoder()\n",
    "pca = PCA(n_components=3)  # Change this to 3 for 3D plotting\n",
    "X_reduced = pca.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Add a 3D subplot\n",
    "\n",
    "# Scatter plot for 3D data\n",
    "scatter = ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y_test_numeric, cmap='viridis')\n",
    "\n",
    "# Color bar\n",
    "plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "# Generate legend manually\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "ax.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062d36b-3943-498c-a0f6-9e6fd02178fe",
   "metadata": {},
   "source": [
    "### General t-SNE Explanation\n",
    "\n",
    "**t-SNE Overview**: t-SNE is a non-linear dimensionality reduction technique that is particularly well suited for embedding high-dimensional data into a space of two or three dimensions. The technique is designed to maintain the local structure of the data, making it excellent for visualizing clusters or groups within the data.\n",
    "\n",
    "- **Local Structure Preservation**: t-SNE focuses on preserving the local distances between points, meaning that points that are similar in the high-dimensional space are placed near each other in the reduced space. However, global distances (i.e., distances between clusters) might not be as accurately represented.\n",
    "- **Interpretation of Axes**: Unlike PCA, t-SNE axes do not have intrinsic meaning as principal components do. The axes in t-SNE plots are abstract and do not correspond to specific original variables. The placement and orientation of clusters can vary significantly between different runs of the algorithm due to its stochastic nature. The axes are simply the 2D or 3D coordinates chosen to best preserve local point-to-point distances.\n",
    "\n",
    "### Specific to 2D Plots\n",
    "\n",
    "In a 2D t-SNE plot:\n",
    "\n",
    "- **X-axis and Y-axis**: These represent the two dimensions onto which the data has been mapped. The axes themselves don’t carry specific meanings but serve as a canvas to observe the grouping and separation of data points.\n",
    "\n",
    "**Visual Analysis**: 2D plots are typically sufficient to identify clusters and outliers. They allow for easy visualization of how data points are grouped and which points are similar to each other.\n",
    "\n",
    "### Specific to 3D Plots\n",
    "\n",
    "A 3D t-SNE plot introduces an additional dimension:\n",
    "\n",
    "- **Z-axis**: Adds depth to the visualization, offering another layer for interpreting the data. This can sometimes reveal structures hidden in 2D views.\n",
    "\n",
    "**Enhanced Visual Analysis**: 3D plots can provide a more comprehensive view of the data's structure, revealing relationships that might not be perceptible in only two dimensions. However, they can also be more challenging to interpret and navigate, especially when trying to understand complex data relationships visually.\n",
    "\n",
    "### Practical Use\n",
    "\n",
    "Both 2D and 3D t-SNE plots are used for:\n",
    "\n",
    "- **Cluster Visualization**: Effectively demonstrates how data points are clustered or grouped, which is invaluable for exploratory data analysis, particularly in fields like genomics, image analysis, and text data analysis.\n",
    "- **Outlier Detection**: Helps in identifying data points that do not fit well with any group.\n",
    "- **Data Exploration**: Provides a means to visually explore the structure of the data, which can guide further analysis or preprocessing steps.\n",
    "\n",
    "t-SNE is a powerful tool for data visualization, especially when the primary interest is to understand the local structure of the data or to discover patterns in data that lacks clear labels or defined groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f24c4c-e2b3-4b84-8629-658ef03bc68d",
   "metadata": {},
   "source": [
    "#### 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b19ef1-0cc5-4e32-9da9-73bc1196336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions (t-SNE)\n",
    "label_encoder = LabelEncoder()\n",
    "tsne = TSNE(n_components=2, random_state=16)\n",
    "X_embedded = tsne.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02054c1-215c-4d35-bff9-9242c81b0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_test_numeric, cmap='viridis')\n",
    "plt.colorbar(scatter) \n",
    "\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "plt.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd8ae-34f1-4e39-82a6-de72457f88f0",
   "metadata": {},
   "source": [
    "#### 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f3835-432a-4cfa-b0e1-e75d8695de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions (t-SNE)\n",
    "label_encoder = LabelEncoder()\n",
    "tsne = TSNE(n_components=3, random_state=16)\n",
    "X_embedded = tsne.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42b18b-8700-477a-9a47-660f28bc5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Add a 3D subplot\n",
    "\n",
    "# Scatter plot for 3D data\n",
    "scatter = ax.scatter(X_embedded[:, 0], X_embedded[:, 1], X_embedded[:, 2], c=y_test_numeric, cmap='viridis')\n",
    "\n",
    "# Adjusting plot limits to exclude outliers\n",
    "xlims = [np.percentile(X_embedded[:, 0], 5), np.percentile(X_embedded[:, 0], 95)]\n",
    "ylims = [np.percentile(X_embedded[:, 1], 5), np.percentile(X_embedded[:, 1], 95)]\n",
    "zlims = [np.percentile(X_embedded[:, 2], 5), np.percentile(X_embedded[:, 2], 95)]\n",
    "\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_zlim(zlims)\n",
    "\n",
    "# Color bar\n",
    "plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "# Generate legend manually\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "ax.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a42610-3880-4233-b570-34a56d59caf3",
   "metadata": {},
   "source": [
    "## Model Coefficients\n",
    "\n",
    "You can also look at the coefficients of the logistic regression model to determine the importance of each feature (word) but visualizing this effectively can be challenging due to the high number of features. You might want to display the most influential words for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffc2b5-c8c4-459e-bdff-eba051bea350",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "for class_index in range(logreg.coef_.shape[0]):  # logreg.coef_.shape[0] gives the number of classes\n",
    "    sorted_coef_index = logreg.coef_[class_index].argsort()\n",
    "    smallest_coefs = feature_names[sorted_coef_index[:10]]\n",
    "    largest_coefs = feature_names[sorted_coef_index[:-11:-1]]\n",
    "\n",
    "    print(f'Class {class_index + 1}')\n",
    "    print(f'Smallest Coefs:\\n{smallest_coefs}\\n')\n",
    "    print(f'Largest Coefs: \\n{largest_coefs}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08689601-d032-4e1f-a4e9-5f5ee39613b1",
   "metadata": {},
   "source": [
    "## Tinkering Around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02fd7e-bda2-4bab-a345-bf49d9456674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model coefficients\n",
    "coefficients = logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7d77c-f508-4d34-a084-12073802afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_coef_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b556c8-7244-4f9c-8413-d372aeced994",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7f268-103e-417b-9ae7-4e04ca1b29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98d412-fc14-4425-9ff3-7a4a9b9c2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_per_class = np.argsort(np.abs(logreg.coef_), axis=1)[:, -10:]  # Adjust the number to your preference for top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522efb7-8f76-4e7e-b857-8ad5030c5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ed711-5b0a-4987-87bb-09a37c110c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top coefficients for a more meaningful visualization\n",
    "top_coefficients = logreg.coef_[:, top_features_per_class.flatten()]\n",
    "\n",
    "top_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba113cac-1d9d-425d-b44d-bea56c3fc8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold to filter significant coefficients\n",
    "threshold = 5  # This value can be adjusted based on model specifics and empirical testing\n",
    "significant_indices = np.where(np.abs(logreg.coef_) > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6605c-0241-42ac-81cd-d7bfa343483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [f\"Class {i+1}\" for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c4225-593b-4be7-932e-714635713144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `feature_names` is a list of actual words corresponding to features and `class_labels` for classes\n",
    "feature_labels = [feature_names[i] for i in significant_indices[1]]\n",
    "mapped_class_labels = [class_labels[i] for i in significant_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961cd488-e28e-46ff-80e7-f462fb82029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count how often each feature is considered significant\n",
    "significant_features = Counter(significant_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a766417-742e-4623-8d82-edc91272599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot significant coefficients\n",
    "significant_coeffs = logreg.coef_[significant_indices]\n",
    "\n",
    "plt.figure(figsize=(20, 10))  # Adjust size for better readability\n",
    "sns.scatterplot(x=feature_labels, y=mapped_class_labels, size=significant_coeffs, hue=significant_coeffs, palette='coolwarm', sizes=(20, 200))\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Classes')\n",
    "plt.title('Impact of Significant Features Across Classes')\n",
    "plt.xticks(rotation=90)  # Rotate feature labels for better readability\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
