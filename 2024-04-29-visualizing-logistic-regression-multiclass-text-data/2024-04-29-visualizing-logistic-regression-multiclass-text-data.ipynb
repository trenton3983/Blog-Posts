{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0516b5-35bd-4142-bfb8-2a0acefdbfe6",
   "metadata": {},
   "source": [
    "The following code is part of a machine learning pipeline that processes, analyzes, and classifies text data from a dataset containing newsgroup posts. Here's a summary of the main steps and components:\n",
    "\n",
    "1. Imports: The script begins by importing necessary Python libraries and modules for data manipulation, machine learning, and plotting. This includes pandas and numpy for data handling, datasets for loading the data, matplotlib for visualization, and several components from scikit-learn for text vectorization, dimensionality reduction, and logistic regression modeling.\n",
    "\n",
    "2. Data Loading: The dataset named \"rungalileo/20_Newsgroups_Fixed\" is loaded using the `datasets` library. This dataset presumably contains posts from 20 different newsgroups, along with labels indicating which newsgroup each post belongs to.\n",
    "\n",
    "3. Data Preprocessing:\n",
    "\n",
    "    - The dataset is filtered to remove any entries where either the label or the text is missing.\n",
    "    - The 'id' column is removed from both the training and test sets, as it's not relevant for the analysis.\n",
    "    - The labels (`y_train` and `y_test`) are extracted from both the training and test datasets.\n",
    "\n",
    "4. Text Vectorization:\n",
    "\n",
    "    - A `TfidfVectorizer` is used to convert the text data into a matrix of TF-IDF features. Initially, this is done for the test data to display the feature names and vectorized data.\n",
    "    - A second `TfidfVectorizer` is configured with specific thresholds (`min_df=5`, `max_df=0.40`) to vectorize the training text data and then applied to both the training and test datasets to prepare them for modeling.\n",
    "\n",
    "5. Model Training and Prediction:\n",
    "\n",
    "    - A logistic regression model is set up with a specific random state and a maximum iteration count equal to the number of rows in the training dataset.\n",
    "    - The logistic regression model is trained on the vectorized training data and the corresponding labels.\n",
    "    - Finally, the model is used to predict the labels for the vectorized test data.\n",
    "\n",
    "This process involves typical steps in a text classification task, including data cleaning, feature extraction through TF-IDF vectorization, and classification using logistic regression. The purpose is to classify text posts into one of the 20 newsgroups based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadc316-f78d-4fc9-86f8-94f7bdfa63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports at the top\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "dataset = datasets.load_dataset(\"rungalileo/20_Newsgroups_Fixed\")\n",
    "\n",
    "# Filter dataset to drop any \"none\" values\n",
    "filtered_train = dataset[\"train\"].filter(lambda x: x['label'] is not None and x['text'] is not None)\n",
    "filtered_test = dataset[\"test\"].filter(lambda x: x['label'] is not None and x['text'] is not None)\n",
    "\n",
    "# Remove irrelevant 'id' column\n",
    "filtered_test = filtered_test.remove_columns('id')\n",
    "filtered_train = filtered_train.remove_columns('id')\n",
    "\n",
    "y_train = filtered_train['label']\n",
    "y_test = filtered_test['label']\n",
    "\n",
    "# Extract text data from filtered_test\n",
    "text = [item['text'] for item in filtered_test]\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "# Display feature names and vectorized data\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "\n",
    "# Define frequency vectorizer with specific thresholds\n",
    "freqV = TfidfVectorizer(min_df=5, max_df=0.40)\n",
    "tokenized_train = [filtered_train[i]['text'] for i in range(filtered_train.num_rows)]\n",
    "x_train = freqV.fit_transform(tokenized_train)\n",
    "\n",
    "# Apply the same vectorization to test data\n",
    "tokenized_test = [filtered_test[i]['text'] for i in range(filtered_test.num_rows)]\n",
    "x_test = freqV.transform(tokenized_test)\n",
    "\n",
    "# Set up and train the logistic regression model\n",
    "logreg = LogisticRegression(random_state=16, max_iter=filtered_train.num_rows)\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "log_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52ab13-4516-4f4a-85f9-27c68d8e613e",
   "metadata": {},
   "source": [
    "## Multiclass Visualization\n",
    "\n",
    "Significant challenges arise when visualizing logistic regression results for a multiclass classification task, especially with text data transformed via [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) vectorization. Traditional 2D or 3D plots used in binary logistic regression, [such as this example](https://raw.githubusercontent.com/trenton3983/Blog-Posts/main/2024-04-29-visualizing-logistic-regression-multiclass-text-data/82L2cb2T.png), are not directly applicable because they typically represent binary outcomes and cannot easily accommodate the high-dimensional space created by TF-IDF vectorization.\n",
    "\n",
    "### Key Challenges:\n",
    "\n",
    "1. **High Dimensionality**: TF-IDF vectorization transforms text into a high-dimensional space (often thousands of dimensions), where each dimension corresponds to a specific word's frequency or importance in the text. Standard binary logistic regression plots, which typically show decision boundaries in two or three dimensions, cannot naturally extend to this high-dimensional space.\n",
    "\n",
    "2. **Multiclass Classification**: Binary logistic regression is inherently designed for two classes. In multiclass settings, logistic regression models typically use schemes like [one-vs-rest](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) (OvR) or [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), which involve multiple binary decisions or probability distributions across more than two classes. This complexity makes it challenging to depict decision boundaries or class separations in a simple 2D or 3D plot.\n",
    "\n",
    "### Alternative Visualization Strategies:\n",
    "\n",
    "Given these constraints, alternative visualization methods are recommended:\n",
    "\n",
    "- [**Confusion Matrix**](https://en.wikipedia.org/wiki/Confusion_matrix): This is particularly useful in multiclass settings to show the model's performance across all classes, illustrating how often each class is correctly predicted versus misclassified.\n",
    "\n",
    "- [**Dimensionality Reduction**](https://en.wikipedia.org/wiki/Dimensionality_reduction): Techniques such as [PCA](https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis_(PCA)) (Principal Component Analysis) or [t-SNE](https://en.wikipedia.org/wiki/Dimensionality_reduction#t-SNE) (t-Distributed Stochastic Neighbor Embedding) can be used to reduce the dataset to two or three dimensions. These reduced dimensions can then be visualized in a scatter plot, providing a way to observe data clustering and separation at a high level.\n",
    "\n",
    "These methods provide more meaningful insights into the performance and behavior of logistic regression models in multiclass, high-dimensional scenarios like those involving TF-IDF vectorized text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a9e49-4b58-47a5-bc97-b2aef92a7627",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix can help you understand the performance of your classifier across different classes. It shows the actual vs. predicted classifications.\n",
    "\n",
    "In a confusion matrix for a classification task, the matrix isn't necessarily symmetrical, meaning the upper and lower sections (above and below the diagonal) aren't expected to be mirror images of each other. Hereâ€™s why:\n",
    "\n",
    "### Understanding the Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the counts of predictions versus the actual labels:\n",
    "- **Diagonal elements** show the number of correct predictions for each class (True Positives for each class).\n",
    "- **Off-diagonal elements** show the misclassifications:\n",
    "  - **Elements above the diagonal** indicate how many times class X was incorrectly predicted as class Y.\n",
    "  - **Elements below the diagonal** indicate how many times class Y was incorrectly predicted as class X.\n",
    "\n",
    "### Reasons for Asymmetry\n",
    "\n",
    "1. **Class Imbalance**: If some classes have more samples than others, the likelihood of predicting the majority class increases, affecting the symmetry.\n",
    "\n",
    "2. **Model Biases and Sensitivities**: The model might be better at recognizing certain classes over others due to inherent biases in the training data or differences in feature distinctiveness between classes.\n",
    "\n",
    "3. **Error Types**:\n",
    "  - [**Type I Errors (False Positives)**](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error): Cases where the model incorrectly predicts the positive class.\n",
    "  - [**Type II Errors (False Negatives)**](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_II_error): Cases where the model fails to predict the positive class when it is the actual class.\n",
    "    Each type of error might be more common for certain classes than others.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose you have three classes: A, B, and C. If:\n",
    "- Class A is often confused with Class B, but not vice versa.\n",
    "- Class C is often mistaken for both A and B, but rarely are A or B mistaken for C.\n",
    "\n",
    "This leads to a non-symmetrical confusion matrix because the misclassification patterns are not uniform across classes.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "A non-symmetrical confusion matrix is typical in practice, especially in multi-class scenarios where varying features, class distributions, and model sensitivities contribute to unique patterns of misclassification. This matrix is a valuable tool for identifying how well the model performs on each class and where it may need improvements or adjustments in its training data or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ef7f4-5d43-4213-ae68-e04ede3559ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, log_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(ax=ax, cmap='viridis', xticks_rotation='vertical')  # You can specify the color map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b4f49-7907-49b9-bcb3-b9d5aa4a7c35",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction for Visualization\n",
    "\n",
    "You can use techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of your TF-IDF vectors to two or three dimensions, and then plot these dimensions.\n",
    "\n",
    "### General PCA Explanation\n",
    "\n",
    "**Principal Components**: Each principal component is a linear combination of the original features and represents a direction in the feature space where the data varies the most. Principal components are orthogonal to each other and are ordered by the amount of variance they capture from the data.\n",
    "\n",
    "- **Variance Representation**: The first principal component captures the most variance, the second captures the next most, and so on, under the constraint that each is orthogonal to the others.\n",
    "- **Interpretation of Axes**: The axes in PCA plots (whether 2D or 3D) represent these principal components. The ticks on these axes indicate the scale or magnitude of the data points along the respective principal components. Since the data often undergoes transformation such as scaling to have zero mean and unit variance before applying PCA, these ticks do not represent the original units of measurement but rather relative positions in the transformed space.\n",
    "\n",
    "### Specific to 2D Plots\n",
    "\n",
    "In a 2D PCA plot:\n",
    "\n",
    "- **X-axis (First Principal Component)**: Represents the direction of the greatest variance in the data. This axis captures the largest amount of information (variation) in the dataset.\n",
    "- **Y-axis (Second Principal Component)**: Represents the direction of the second greatest variance, orthogonal to the first principal component.\n",
    "\n",
    "**Visual Analysis**: The plot can reveal clustering and other patterns, helping to visually assess similarities and differences in the data. Points that are close together can be interpreted as having similar characteristics according to the most significant features.\n",
    "\n",
    "### Specific to 3D Plots\n",
    "\n",
    "In addition to the first and second principal components, 3D PCA plots include:\n",
    "\n",
    "- **Z-axis (Third Principal Component)**: This axis captures the third highest variance in the data, providing an additional dimension of analysis which is orthogonal to both the first and second components.\n",
    "\n",
    "**Enhanced Visual Analysis**: A 3D plot allows for a deeper visual exploration, revealing structures and relationships that might not be visible in 2D. It can be especially useful in datasets where the top two components do not capture the majority of variance.\n",
    "\n",
    "### Practical Use\n",
    "\n",
    "Both 2D and 3D PCA plots are used for:\n",
    "- **Pattern Recognition**: Identifying clusters or outliers in the data.\n",
    "- **Data Simplification**: Reducing the dimensionality of the data while attempting to retain the most important characteristics.\n",
    "- **Exploratory Data Analysis**: Providing insights into the structure of the data before applying more complex models.\n",
    "\n",
    "These visualizations serve as powerful tools for initial data analysis, especially when dealing with complex datasets like those generated from text vectorization (e.g., TF-IDF) in natural language processing tasks. They help in making informed decisions about the next steps in the data analysis or machine learning workflow.\n",
    "\n",
    "#### 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b125f-fd86-4732-822e-76c2f6b27d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions (PCA)\n",
    "label_encoder = LabelEncoder()\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_test_numeric, cmap='viridis')\n",
    "plt.colorbar(scatter) \n",
    "\n",
    "# Generate legend\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "plt.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a47d18-5698-4c80-bec7-3c920ecbca0a",
   "metadata": {},
   "source": [
    "#### 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e4ef2-89ac-47e3-8467-e9648351e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform with PCA for 3 components\n",
    "label_encoder = LabelEncoder()\n",
    "pca = PCA(n_components=3)  # Change this to 3 for 3D plotting\n",
    "X_reduced = pca.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Add a 3D subplot\n",
    "\n",
    "# Scatter plot for 3D data\n",
    "scatter = ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y_test_numeric, cmap='viridis')\n",
    "\n",
    "# Color bar\n",
    "plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "# Generate legend manually\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "ax.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062d36b-3943-498c-a0f6-9e6fd02178fe",
   "metadata": {},
   "source": [
    "### General t-SNE Explanation\n",
    "\n",
    "**t-SNE Overview**: t-SNE is a non-linear dimensionality reduction technique that is particularly well suited for embedding high-dimensional data into a space of two or three dimensions. The technique is designed to maintain the local structure of the data, making it excellent for visualizing clusters or groups within the data.\n",
    "\n",
    "- **Local Structure Preservation**: t-SNE focuses on preserving the local distances between points, meaning that points that are similar in the high-dimensional space are placed near each other in the reduced space. However, global distances (i.e., distances between clusters) might not be as accurately represented.\n",
    "- **Interpretation of Axes**: Unlike PCA, t-SNE axes do not have intrinsic meaning as principal components do. The axes in t-SNE plots are abstract and do not correspond to specific original variables. The placement and orientation of clusters can vary significantly between different runs of the algorithm due to its stochastic nature. The axes are simply the 2D or 3D coordinates chosen to best preserve local point-to-point distances.\n",
    "\n",
    "### Specific to 2D Plots\n",
    "\n",
    "In a 2D t-SNE plot:\n",
    "\n",
    "- **X-axis and Y-axis**: These represent the two dimensions onto which the data has been mapped. The axes themselves donâ€™t carry specific meanings but serve as a canvas to observe the grouping and separation of data points.\n",
    "\n",
    "**Visual Analysis**: 2D plots are typically sufficient to identify clusters and outliers. They allow for easy visualization of how data points are grouped and which points are similar to each other.\n",
    "\n",
    "### Specific to 3D Plots\n",
    "\n",
    "A 3D t-SNE plot introduces an additional dimension:\n",
    "\n",
    "- **Z-axis**: Adds depth to the visualization, offering another layer for interpreting the data. This can sometimes reveal structures hidden in 2D views.\n",
    "\n",
    "**Enhanced Visual Analysis**: 3D plots can provide a more comprehensive view of the data's structure, revealing relationships that might not be perceptible in only two dimensions. However, they can also be more challenging to interpret and navigate, especially when trying to understand complex data relationships visually.\n",
    "\n",
    "### Practical Use\n",
    "\n",
    "Both 2D and 3D t-SNE plots are used for:\n",
    "\n",
    "- **Cluster Visualization**: Effectively demonstrates how data points are clustered or grouped, which is invaluable for exploratory data analysis, particularly in fields like genomics, image analysis, and text data analysis.\n",
    "- **Outlier Detection**: Helps in identifying data points that do not fit well with any group.\n",
    "- **Data Exploration**: Provides a means to visually explore the structure of the data, which can guide further analysis or preprocessing steps.\n",
    "\n",
    "t-SNE is a powerful tool for data visualization, especially when the primary interest is to understand the local structure of the data or to discover patterns in data that lacks clear labels or defined groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f24c4c-e2b3-4b84-8629-658ef03bc68d",
   "metadata": {},
   "source": [
    "#### 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b19ef1-0cc5-4e32-9da9-73bc1196336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions (t-SNE)\n",
    "label_encoder = LabelEncoder()\n",
    "tsne = TSNE(n_components=2, random_state=16)\n",
    "X_embedded = tsne.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02054c1-215c-4d35-bff9-9242c81b0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_test_numeric, cmap='viridis')\n",
    "plt.colorbar(scatter) \n",
    "\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "plt.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd8ae-34f1-4e39-82a6-de72457f88f0",
   "metadata": {},
   "source": [
    "#### 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffe050-8379-4275-a31a-61623d5824fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions (t-SNE)\n",
    "label_encoder = LabelEncoder()\n",
    "tsne = TSNE(n_components=3, random_state=16)\n",
    "X_embedded = tsne.fit_transform(x_test.toarray())\n",
    "y_test_numeric = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10087d-235c-4c26-81cd-7c361c370b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Add a 3D subplot\n",
    "\n",
    "# Scatter plot for 3D data\n",
    "scatter = ax.scatter(X_embedded[:, 0], X_embedded[:, 1], X_embedded[:, 2], c=y_test_numeric, cmap='viridis')\n",
    "\n",
    "# Color bar\n",
    "plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "# Generate legend manually\n",
    "classes = label_encoder.classes_\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=5) for col in colors]\n",
    "ax.legend(legend_handles, classes, title=\"Classes\", bbox_to_anchor=(1.2, 0.5), loc='center left', frameon=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a42610-3880-4233-b570-34a56d59caf3",
   "metadata": {},
   "source": [
    "## Model Coefficients\n",
    "\n",
    "You can also look at the coefficients of the logistic regression model to determine the importance of each feature (word) but visualizing this effectively can be challenging due to the high number of features. You might want to display the most influential words for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c24a6-11b7-45d2-ac64-5739114bc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "sorted_coef_index = logreg.coef_[0].argsort()\n",
    "\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
